# Cameron Zachreson
### date: April 30th, 2023

I will attempt to implement the exercise as published on April 28th, 2023

## some potential issues I aim to address: 
lack of specificity in exercise prompts
lack of generality in testing protocol (i.e., specific to python) 


# Exercise 1: 

current prompt: Define a reproducible environment in which the model can run

## CZ implementation: 

### OS and IDE

 - Operating System: Windows 10 Version 21H2 (OS Build 19044.2846)
 - IDE: Spyder version 4.2.5

### Packages

 - numpy 1.20.1

### new code: 
implemented some functions to output build info. The script 
```sh
build_info.py
```
contains functions for printing information about the operation system, the python version, and any list of modules that contain the 
```sh
.__version__
```
attribute. I've added some of these to 
```sh
test.py
```
so that the running OS, python, and numpy versions are printed to console. It would be good to print these into an output file generated by the test script. This can be part of Exercise 2. 


# Exercise 2: 

current prompt: Update the model so that its outputs can be reproduced

## CZ Implementation 

### reproducibility issues: 
 - the random number generator is set to the default, which is OK, but it would be good to print out any available details
 - the rng seed is not defined, which means each run will produce different output
 - regarding the above, the number of samples is small, so the model will sometimes pass the error tests, and sometimes not. 

### potential solutions: 
 - produce a configuration file that gives build info and some specific details about the model (i.e., what seed and rng are used) 
 - perform a test that compares a previously-generated sample set, to a new one that uses the same seed and sample size
 - note that there is no simple solution for the statistical tests. The tolerance and sample size are subjective, and there is no guarantee that these tests will be able to distinguish intrinsic variation (i.e., different results of the same model) from variation produced by model error (i.e., similar results from an incorrect impelementation of the same model or from an entirely different model). However, it makes sense to ensure that the tolerance is such that the same model run with different rng seeds will (almost always) pass the tests. 

### updates: 
 - moved model input parameters (and test parameters) to defined variables
 - added an output configuration file that stores the model input parameters, build info, and test results
 - built a test-case generator that stores a copy of the full model output for a given test case 
 - removed hard-coded inputs and implemented read-in model parameters from input config file


# Exercise 3: 

current prompt: Write test cases that check if the model outputs are reproducible

## CZ Implementation 

### current solution: 
 - produced a modified version of 
```sh
test.py
```
called 
```sh
generate_test_cases.py
```
which reads in a parameter file, generates model output, and then saves the input, output, and build data into a directory with a label for the test case (here I used CZv0). 
- the test script now specifies a test case label (i.e., CZv0 as a string) which it uses to locate the list of parameters and test values for calling the model function and comparing the output generated to that of the test case. 
- I implemented a new test case, which compares each element of the model output to the test case. A precision (decimal place) is specified, and if all value pairs [generated value, test case value] pass an equality check, then the test passes. This ensures that
 -- the model being tested is specified by the same set of input arguments used in the test case 
 -- the model generates the same output, identical to a specified precision.   

